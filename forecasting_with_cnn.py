# -*- coding: utf-8 -*-
"""Forecasting_with_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KxceBjeDl5ao-0gjNfDLI0lmJNTjnvTZ

# Imports
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

keras = tf.keras

# Set default figsize to 10 inches by 6 inches
plt.rcParams["figure.figsize"] = (10,6)

"""# Functions"""

def plot_series(time,series,format="-",start=0,end=None,label=None):
  plt.plot(time[start:end],series[start:end])
  plt.xlabel("Time")
  plt.ylabel("Value")
  if label:
    plt.legend(fontsize=14)
  plt.grid(True)

def trend(time,slope=0):
  return time * slope

def seasonal_pattern(season_time):
  return np.where(season_time < 0.4,
                  np.cos(season_time * 2 * np.pi),
                  1 / np.exp(3 * season_time))

def seasonality(time,period,amplitude=1,phase =0):
  season_time = ((time + phase)) % period / period
  return amplitude * seasonal_pattern(season_time)

def white_noise(time,noise_level =1 , seed = None):
  rnd = np.random.RandomState(seed)
  return rnd.randn(len(time)) * noise_level

def seq2seq_window_dataset(series, window_size, batch_size = 32,
                           shuffle_buffer = 1000):
  series = tf.expand_dims(series,axis=-1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1,shift=1,drop_remainder = True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1],w[1:]))
  return ds.batch(batch_size).prefetch(1)

def model_forecast(model,series,window_size,batch_size= 32):
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size,shift = 1,drop_remainder = True)
  ds = ds.flat_map(lambda w: w.batch(window_size))
  ds = ds.batch(batch_size).prefetch(1)
  forecast = model.predict(ds)
  return forecast

time = np.arange(4 * 365 + 1)

slope = 0.05
baseline = 10
amplitude = 40
period = 365

series = baseline + trend(time,slope) + seasonality(time,period,amplitude=amplitude)

noise_level = 5
noise = white_noise(time,noise_level,seed=42)

series += noise

plot_series(time,series)
plt.show()

split_time = 1000
time_train = time[:split_time]
x_train = series[:split_time]

valid_time = time[split_time:]
x_valid = series[split_time:]

"""# Preprocessing with A 1d CNN"""

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# 
# keras.backend.clear_session()
# 
# tf.random.set_seed(42)
# np.random.seed(42)
# 
# window_size = 30
# 
# train_set = seq2seq_window_dataset(x_train,window_size,batch_size=128)
# 
# model = keras.models.Sequential([
#   keras.layers.Conv1D(filters= 32,kernel_size=5,
#                       strides=1,padding='causal',
#                       activation='relu',
#                       input_shape=[None,1]),
#   keras.layers.LSTM(32,return_sequences=True),
#   keras.layers.LSTM(32,return_sequences=True),
#   keras.layers.Dense(2),
#   keras.layers.Lambda(lambda x: x * 200.0)
# ])
# 
# lr_schedule = keras.callbacks.LearningRateScheduler(
#     lambda epoch: 1e-8 * 10 **(epoch/20))
# 
# optimizer = keras.optimizers.SGD(lr=1e-8,momentum=0.9)
# 
# model.compile(loss = keras.losses.Huber(),
#               optimizer=optimizer,
#               metrics=['mae'])
# 
# history = model.fit(train_set,epochs=100,callbacks=[lr_schedule])

# Plot learning rate curve
plt.semilogx(history.history['lr'],history.history['loss'])
plt.axis([1e-8,1e-4,0,30])

"""# Now that we got the learning rate curve lets create our actual model"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# keras.backend.clear_session()
# tf.random.set_seed(42)
# np.random.seed(42)
# 
# window_size = 30
# batch_size = 128
# 
# train_set = seq2seq_window_dataset(x_train,window_size,batch_size=batch_size)
# valid_set = seq2seq_window_dataset(x_valid,window_size,batch_size=batch_size)
# 
# model = keras.models.Sequential([
#     keras.layers.Conv1D(filters = 32, kernel_size = 5,
#                         strides = 1, padding="causal",
#                         activation = 'relu',
#                         input_shape=[None,1]),
#     keras.layers.LSTM(32,return_sequences=True),
#     keras.layers.LSTM(32,return_sequences=True),
#     keras.layers.Dense(2),
#     keras.layers.Lambda(lambda x: x * 200)
# ])
# optimizer = keras.optimizers.SGD(lr=1e-5,momentum=0.9)
# model.compile(loss = keras.losses.Huber(),
#               optimizer = optimizer,
#               metrics=['mae'])
# model_checkpoint = keras.callbacks.ModelCheckpoint(
#     "my_checkpoint.h5",save_best_only = True
# )
# early_stopping = keras.callbacks.EarlyStopping(patience=40)
# 
# model.fit(train_set,
#           epochs=500,
#           validation_data = valid_set,
#           callbacks=[early_stopping,model_checkpoint])

model = keras.models.load_model("my_checkpoint.h5")

rnn_forecast = model_forecast(model, series[:,  np.newaxis], window_size)
rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]

plot_series(valid_time,x_valid)
plot_series(valid_time,rnn_forecast)

keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()

"""# Fully Convolutional Forecasting with dilationg"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# keras.backend.clear_session()
# tf.random.set_seed(42)
# np.random.seed(42)
# 
# window_size = 64
# batch_size = 128
# 
# train_set = seq2seq_window_dataset(x_train,window_size,
#                                    batch_size=batch_size)
# 
# model = keras.models.Sequential()
# model.add(keras.layers.InputLayer(input_shape=[None,1]))
# 
# for dilation_rate in (1,2,4,8,16,32):
#   model.add(keras.layers.Conv1D(filters=32,
#                                 kernel_size=2,
#                                 strides=1,
#                                 dilation_rate=dilation_rate,
#                                 padding = 'causal',
#                                 activation = 'relu'))
# model.add(keras.layers.Conv1D(filters=1,kernel_size=1))
# lr_schedule = keras.callbacks.LearningRateScheduler(
#     lambda epoch:1e-4 * 10 ** (epoch/30)
# )
# optimizer = keras.optimizers.Adam(lr=1e-4)
# 
# model.compile(loss = keras.losses.Huber(),
#               optimizer=optimizer,
#               metrics=['mae'])
# 
# history = model.fit(train_set,epochs = 100,callbacks=[lr_schedule])

plt.semilogx(history.history['lr'],history.history['loss'])
plt.axis([1e-4,1e-1,0,30])

# Commented out IPython magic to ensure Python compatibility.
# # Since we now have the best learning rate lets create our actual model
# %%time
# 
# keras.backend.clear_session()
# tf.random.set_seed(42)
# np.random.seed(42)
# 
# window_size = 64
# batch_size = 128
# 
# train_set = seq2seq_window_dataset(x_train,window_size,batch_size=batch_size)
# 
# valid_set = seq2seq_window_dataset(x_valid,window_size=window_size,batch_size=batch_size)
# 
# model = keras.models.Sequential()
# model.add(keras.layers.InputLayer(input_shape=[None,1]))
# 
# for dilation_rate in (1,2,3,8,16,32):
#   model.add(
#       keras.layers.Conv1D(filters=32,
#                           kernel_size=2,
#                           strides=1,
#                           dilation_rate=dilation_rate,
#                           padding='causal',
#                           activation='relu')
#   )
# model.add(keras.layers.Conv1D(filters=1,kernel_size=1))
# optimizer = keras.optimizers.Adam(lr=3e-4)
# model.compile(loss= keras.losses.Huber(),
#               optimizer=optimizer,
#               metrics=['mae'])
# model_checkpoint = keras.callbacks.ModelCheckpoint(
#     "my_checkpoint.h5",save_best_only=True)
# early_stopping = keras.callbacks.EarlyStopping(patience=50)
# history = model.fit(train_set,
#                     epochs=500,
#                     validation_data = valid_set,
#                     callbacks=[early_stopping,model_checkpoint])

model = keras.models.load_model("my_checkpoint.h5")

cnn_forecast = model_forecast(model,series[...,np.newaxis],window_size)
cnn_forecast = cnn_forecast[split_time - window_size:-1,-1,0]

plot_series(valid_time,x_valid)
plot_series(valid_time,cnn_forecast)

keras.metrics.mean_absolute_error(x_valid, cnn_forecast).numpy()

